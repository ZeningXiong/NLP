2025-04-27 22:55:34,253 - INFO - Saved hyperparameters to ./logs\run_20250427_225534_707e\hyperparameters.json
2025-04-27 22:55:34,253 - INFO - Loading model: Qwen/Qwen2.5-Math-1.5B-Instruct
2025-04-27 22:55:34,254 - INFO - Loading model from cache: /root/autodl-tmp/model/Qwen/Qwen2.5-Math-1.5B-Instruct
2025-04-27 22:55:34,922 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-27 22:55:41,474 - INFO - Configuring LoRA...
2025-04-27 22:55:41,570 - INFO - LoRA configuration applied successfully
2025-04-27 22:55:41,573 - INFO - Initialized TensorBoard writer at ./logs\run_20250427_225534_707e
2025-04-27 22:55:41,574 - INFO - Loading dataset...
2025-04-27 22:55:42,648 - INFO - Starting training...
2025-04-27 22:55:58,530 - ERROR - Training failed: CUDA out of memory. Tried to allocate 3.23 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 20.97 GiB is allocated by PyTorch, and 372.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-27 22:55:58,532 - INFO - Closed TensorBoard writer
